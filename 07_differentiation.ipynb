{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table>\n",
    " <tr align=left><td><img align=left src=\"./images/CC-BY.png\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license. (c) Kyle T. Mandli</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Differentiation\n",
    "\n",
    "**GOAL:**  Given a set of $N+1$ points $(x_i, y_i)$ compute the derivative of a given order at a point $x$ to a specified accuracy.\n",
    "\n",
    "**Approaches:** \n",
    " * Find the interpolating polynomial $P_N(x)$ and differentiate that.\n",
    " * Use Taylor-series expansions and the method of undetermined coefficients to derive finite-difference weights and their error estimates\n",
    " \n",
    "**Issues:**  Order vs accuracy...how to choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example 1:  how to approximate the derivative $f'(x)$ given a discrete sampling of a function $f(x)$\n",
    "\n",
    "Here we will consider how to estimate $f'(x_k)$ given a $N$ point sampling of $f(x)=\\sin(\\pi x) + 1/2 \\sin(2\\pi x)$ sampled uniformly over the interval $x\\in [ 0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "N = 11\n",
    "x = numpy.linspace(0,1,N)\n",
    "xfine = numpy.linspace(0,1,101)\n",
    "f = lambda x:  numpy.sin(numpy.pi*x) + 0.5*numpy.sin(4*numpy.pi*x)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "axes = fig.add_subplot(1,1,1)\n",
    "axes.plot(xfine, f(xfine),'b',label='$f(x)$')\n",
    "axes.plot(x, f(x), 'ro', markersize=12, label='$f(x_k)$')\n",
    "axes.grid()\n",
    "axes.set_xlabel('x')\n",
    "p = numpy.polyfit(x,f(x),N-1)\n",
    "axes.plot(xfine,numpy.polyval(p,xfine),'g--',label='$P_{{{N}}}$'.format(N=N-1))\n",
    "axes.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example 2:  how to approximate derivative $f'(x)$ given a discrete sampling of a function $f(x)$\n",
    "\n",
    "Here we will consider how to estimate $f'(x_k)$ given a $N$ point sampling of Runge's function sampled uniformly over the interval $x\\in [ -1,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "N = 11\n",
    "x = numpy.linspace(-1,1,N)\n",
    "xfine = numpy.linspace(-1,1,101)\n",
    "f = lambda x:  1./(1. + 25*x**2)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "axes = fig.add_subplot(1,1,1)\n",
    "axes.plot(xfine, f(xfine),'b',label='$f(x)$')\n",
    "axes.plot(x, f(x), 'ro', markersize=12, label='$f(x_k)$')\n",
    "axes.grid()\n",
    "axes.set_xlabel('x')\n",
    "p = numpy.polyfit(x,f(x),N-1)\n",
    "axes.plot(xfine,numpy.polyval(p,xfine),'g--',label='$P_{{{N}}}$'.format(N=N-1))\n",
    "axes.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The interpolating polynomial: review\n",
    "\n",
    "From our previous lecture, we showed that we can approximate a function $f(x)$ over some interval in terms of a unique interpolating polynomial through $N+1$ points and a  remainder term\n",
    "\n",
    "$$\n",
    "    f(x) = P_N(x) + R_N(x)\n",
    "$$\n",
    "\n",
    "Where the Lagrange remainder term is\n",
    "\n",
    "$$R_N(x) = (x - x_0)(x - x_1)\\cdots (x - x_{N}) \\frac{f^{(N+1)}(c)}{(N+1)!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While there are multiple ways to represent the interpolating polynomial, both $P_N(x)$ and $R_N(x)$ are polynomials in $x$ and therefore differentiable.  Thus we should be able to calculate the first derivative and its error as\n",
    "\n",
    "$$\n",
    "    f'(x) = P'_N(x) + R'_N(x)\n",
    "$$\n",
    "\n",
    "and likewise for higher order derivatives up to degree $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Derivatives of the Lagrange Polynomials \n",
    "\n",
    "The Lagrange basis, is a particularly nice basis for calculating numerical differentiation formulas because of their basic interpolating property that\n",
    "\n",
    "$$\n",
    "    P_N(x) = \\sum_{i=0}^N f(x_i)\\ell_i(x)\n",
    "$$\n",
    "\n",
    "where $f(x_i)$ is just the value of our function $f$ at node $x_i$ and all of the $x$ dependence is contained in the Lagrange Polynomials $\\ell_i(x)$ (which only depend on the node coordinates $x_i$, $i=0,\\ldots,N$).  Thus, the interpolating polynomial at any $x$ is simply a linear combination of the values at the nodes $f(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Likewise its first derivative\n",
    "$$\n",
    "P'_N(x)  = \\sum_{i=0}^N f(x_i)\\ell'_i(x)\n",
    "$$\n",
    "is also just a linear combination of the values $f(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples\n",
    "\n",
    "Given the potentially, highly oscillatory nature of the interpolating polynomial, in practice we only use a small number of data points around a given point $x_k$ to derive a differentiation formula for the derivative $f'(x_k)$.  In the context of differential equations we also often have $f(x)$ so that $f(x_k) = y_k$ and we can approximate the derivative of a known function $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "N = 9\n",
    "f = lambda x:  1./(1. + 25*x**2)\n",
    "#f = lambda x: numpy.cos(2.*numpy.pi*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x = numpy.linspace(-1,1,N)\n",
    "xfine = numpy.linspace(-1,1,101)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "axes = fig.add_subplot(1,1,1)\n",
    "axes.plot(xfine, f(xfine),'b',label='$f(x)$')\n",
    "axes.plot(x, f(x), 'ro', markersize=12, label='$f(x_k)$')\n",
    "x3 = x[5:8]\n",
    "x3fine = numpy.linspace(x3[0],x3[-1],20)\n",
    "p = numpy.polyfit(x3,f(x3),2)\n",
    "axes.plot(x3,f(x3),'m',label = 'Piecewise $P_1(x)$')\n",
    "axes.plot(x3fine,numpy.polyval(p,x3fine),'k',label = 'Piecewise $P_2(x)$')\n",
    "axes.grid()\n",
    "axes.set_xlabel('x')\n",
    "p = numpy.polyfit(x,f(x),N-1)\n",
    "axes.plot(xfine,numpy.polyval(p,xfine),'g--',label='$P_{{{N}}}$'.format(N=N-1))\n",
    "axes.legend(fontsize=14,loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: 1st order polynomial through 2 points $x=x_0, x_1$:\n",
    "\n",
    "\n",
    "$$\n",
    "    P_1(x)=f_0\\ell_0(x) + f_1\\ell_1(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or written out in full\n",
    "\n",
    "$$\n",
    "P_1(x) = f_0\\frac{x-x_1}{x_0-x_1} + f_1\\frac{x-x_0}{x_1-x_0} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus the first derivative of this polynomial for all $x\\in[x_0,x_1]$ is\n",
    "\n",
    "$$\n",
    "P'_1(x) = \\frac{f_0}{x_0-x_1} + \\frac{f_1}{x_1-x_0} = \\frac{f_1 - f_0}{x_1 - x_0} = \\frac{f_1 - f_0}{\\Delta x}\n",
    "$$\n",
    "\n",
    "Where $\\Delta x$ is the width of the interval.  This formula is simply the slope of the chord connecting the points $(x_0, f_0)$ and $(x_1,f_1)$.   Note also, that the estimate of the first-derivative is constant for all $x\\in[x_0,x_1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### \"Forward\" and \"Backward\" first derivatives\n",
    "\n",
    "Even though the first derivative by this method is the same at both $x_0$ and $x_1$, we sometime make a distinction between the \"forward Derivative\"\n",
    "\n",
    "$$f'(x_n) \\approx D_1^+ = \\frac{f(x_{n+1}) - f(x_n)}{\\Delta x}$$\n",
    "\n",
    "and the \"backward\" finite-difference as\n",
    "\n",
    "$$f'(x_n) \\approx D_1^- = \\frac{f(x_n) - f(x_{n-1})}{\\Delta x}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N=5\n",
    "x = numpy.linspace(0,1,N)\n",
    "xfine = numpy.linspace(0,1,101)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "axes = fig.add_subplot(1,1,1)\n",
    "axes.plot(xfine, f(xfine),'b',label='$f(x)$')\n",
    "axes.plot(x, f(x), 'ro', markersize=12, label='$f(x_k)$')\n",
    "x3 = x[1:4]\n",
    "x3fine = numpy.linspace(x3[0],x3[-1],20)\n",
    "p = numpy.polyfit(x3,f(x3),2)\n",
    "axes.plot(x3,f(x3),'m',label = 'Piecewise $P_1(x)$')\n",
    "axes.plot(x3fine,numpy.polyval(p,x3fine),'k',label = 'Piecewise $P_2(x)$')\n",
    "axes.grid()\n",
    "axes.set_xlabel('x')\n",
    "axes.legend(fontsize=14,loc='best')\n",
    "axes.text(.6, .15,'$D_1^+$',size=20)\n",
    "axes.text(.38, .3,'$D_1^-$',size=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note these approximations should be familiar to use as the limit as $\\Delta x \\rightarrow 0$ these are no longer approximations but equivalent definitions of the derivative at $x_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: 2nd order polynomial through 3 points $x=x_0, x_1, x_2$:\n",
    "\n",
    "\n",
    "$$\n",
    "    P_2(x)=f_0\\ell_0(x) + f_1\\ell_1(x) + f_2\\ell_2(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or written out in full\n",
    "\n",
    "$$\n",
    "P_2(x) = f_0\\frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)} + f_1\\frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)} + f_2\\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus the first derivative of this polynomial for all $x\\in[x_0,x_2]$ is\n",
    "\n",
    "$$\n",
    "P'_2(x) = f_0\\frac{(x-x_1)+(x-x_2)}{(x_0-x_1)(x_0-x_2)} + f_1\\frac{(x-x_0)+(x-x_2)}{(x_1-x_0)(x_1-x_2)} + f_2\\frac{(x-x_0)+(x-x_1)}{(x_2-x_0)(x_2-x_1)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Exercise**: show that the second-derivative $P''_2(x)$ is a constant (find it!) but is also just a linear combination of the function values at the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Special case of equally spaced nodes $x = [-h, 0, h]$ where $h=\\Delta x$ is the grid spacing\n",
    "\n",
    "\n",
    "General Case:\n",
    "$$\n",
    "P'_2(x) = f_0\\frac{(x-x_1)+(x-x_2)}{(x_0-x_1)(x_0-x_2)} + f_1\\frac{(x-x_0)+(x-x_2)}{(x_1-x_0)(x_1-x_2)} + f_2\\frac{(x-x_0)+(x-x_1)}{(x_2-x_0)(x_2-x_1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Becomes:\n",
    "$$\n",
    "P'_2(x) = f_0\\frac{2x-h}{2h^2} + f_1\\frac{-2x}{h^2} + f_2\\frac{2x+h}{2h^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "which if we evaluate at the three nodes  $-h,0,h$ yields\n",
    "\n",
    "$$\n",
    "P'_2(-h) = \\frac{-3f_0 + 4f_1 -1f_2}{2h}, \\quad\\quad P'_2(0) = \\frac{-f_0 + f_2}{2h}, \\quad\\quad P'_2(h) = \\frac{f_0 -4f_1 + 3f_2}{2h} \n",
    "$$\n",
    "\n",
    "Again, just  linear combinations of the values at the nodes $f(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Quick Checks\n",
    "\n",
    "In general,  all finite difference formulas can be written as linear combinations of the values of $f(x)$ at the nodes.  The formula's can be hard to remember, but they are easy to check.\n",
    "\n",
    "* The sum of the coefficients must add to zero.  Why?\n",
    "* The sign of the coefficients can be checked by inserting $f(x_i) = x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Example\n",
    "\n",
    "Given \n",
    "$$\n",
    "P'_2(-h) =\\frac{-3f_0 + 4f_1 -1f_2}{2h}\n",
    "$$\n",
    "\n",
    "What is $P'_2(-h)$ if\n",
    "\n",
    "* $$f_0=f_1=f_2$$\n",
    "* $$f_0 = 0, ~f_1 = 1, ~f_2 = 2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Error Analysis\n",
    "\n",
    "In addition to calculating finite difference formulas, we can also estimate the error\n",
    "\n",
    "From Lagrange's Theorem,  the remainder term looks like\n",
    "\n",
    "$$R_N(x) = (x - x_0)(x - x_1)\\cdots (x - x_{N})) \\frac{f^{(N+1)}(c)}{(N+1)!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus the derivative of the remainder term $R_N(x)$ is\n",
    "\n",
    "$$R_N'(x) = \\left(\\sum^{N}_{i=0} \\left( \\prod^{N}_{j=0,~j\\neq i} (x - x_j) \\right )\\right ) \\frac{f^{(N+1)}(c)}{(N+1)!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The remainder term contains a sum of $N$'th order polynomials and can be awkward to evaluate, however, if we restrict ourselves to the error at any given node $x_k$, the remainder simplifies to \n",
    "\n",
    "$$R_N'(x_k) = \\left( \\prod^{N}_{j=0,~j\\neq k} (x_k - x_j) \\right) \\frac{f^{(N+1)}(c)}{(N+1)!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we let $\\Delta x = \\max_i |x_k - x_i|$ we then know that the remainder term will be $\\mathcal{O}(\\Delta x^N)$ as $\\Delta x \\rightarrow 0$ thus showing that this approach converges and we can find arbitrarily high order approximations (ignoring floating point error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples\n",
    "\n",
    "#### First order differences $N=1$\n",
    "\n",
    "For our first order finite differences, the error term is simply\n",
    "\n",
    "$$R_1'(x_0) = -\\Delta x \\frac{f''(c)}{2}$$\n",
    "$$R_1'(x_1) = \\Delta x \\frac{f''(c)}{2}$$\n",
    "\n",
    "Both of which are $O(\\Delta x f'')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Second order differences $N=2$\n",
    "\n",
    "\n",
    "For general second order polynomial interpolation, the derivative of the remainder term is\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    R_2'(x) &= \\left(\\sum^{2}_{i=0} \\left( \\prod^{2}_{j=0,~j\\neq i} (x - x_j) \\right )\\right ) \\frac{f'''(c)}{3!} \\\\\n",
    "    &= \\left ( (x - x_{i+1}) (x - x_{i-1}) + (x-x_i) (x-x_{i-1}) + (x-x_i)(x-x_{i+1}) \\right ) \\frac{f'''(c)}{3!}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again evaluating this expression at the center point $x = x_i$ and assuming evenly space points we have\n",
    "\n",
    "$$R_2'(x_i) = -\\Delta x^2 \\frac{f'''(c)}{3!}$$\n",
    "\n",
    "showing that our error is $\\mathcal{O}(\\Delta x^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### <font color='red'>Caution</font>\n",
    "\n",
    "High order does not necessarily imply high-accuracy! \n",
    "\n",
    "As always, the question remains as to whether the underlying function is well approximated by a high-order polynomial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Convergence \n",
    "\n",
    "Nevertheless, we can always check to see if the error reduces as expected as $\\Delta x\\rightarrow 0$.  Here we estimate the 1st and 2nd order first-derivative for evenly spaced points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def D1_p(func, x_min, x_max, N):\n",
    "    \"\"\" calculate consistent 1st order Forward difference of a function func(x) defined on the interval [x_min,xmax]\n",
    "    and sampled at N evenly spaced points\"\"\"\n",
    "\n",
    "    x = numpy.linspace(x_min, x_max, N)\n",
    "    f = func(x)\n",
    "    dx = x[1] - x[0]\n",
    "    f_prime = numpy.zeros(N)\n",
    "    f_prime[0:-1] = (f[1:] - f[0:-1])/dx\n",
    "    # and patch up the end point with a backwards difference\n",
    "    f_prime[-1] = f_prime[-2]\n",
    "\n",
    "    return f_prime\n",
    "\n",
    "def D1_2(func, x_min, x_max, N):\n",
    "    \"\"\" calculate consistent 2nd order first derivative of a function func(x) defined on the interval [x_min,xmax]\n",
    "    and sampled at N evenly spaced points\"\"\"\n",
    "\n",
    "    x = numpy.linspace(x_min, x_max, N)\n",
    "    f = func(x)\n",
    "    dx = x[1] - x[0]\n",
    "    f_prime = numpy.zeros(N)\n",
    "    # consistent 2nd order one-sided 1st derivative at x_min\n",
    "    f_prime[0] = f[:3].dot(numpy.array([-3, 4, -1]))/(2*dx)\n",
    "    \n",
    "    # centered derivatives in the interior\n",
    "    f_prime[1:-1] = (f[2:N] - f[0:-2])/(2*dx)\n",
    "    \n",
    "    # consistent 2nd order one-sided 1st derivative at x_min\n",
    "    f_prime[-1] = f[-3:].dot(numpy.array([1, -4, 3]))/(2*dx)\n",
    "    \n",
    "    return f_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Note:  \n",
    "\n",
    "This first derivative operator can also be written as a Matrix $D$ such that $f'(\\mathbf{x}) = Df(\\mathbf{x})$ where $\\mathbf{x}$ is a vector of $x$ coordinates. (exercise left for the homework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N = 21\n",
    "xmin = 0.\n",
    "xmax = 1.\n",
    "func = lambda x:  numpy.sin(numpy.pi*x) + 0.5*numpy.sin(4*numpy.pi*x)\n",
    "func_prime = lambda x: numpy.pi*(numpy.cos(numpy.pi*x) + 2. * numpy.cos(4*numpy.pi*x))\n",
    "D1f = D1_p(func, xmin, xmax, N)\n",
    "D2f = D1_2(func, xmin, xmax, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "xa = numpy.linspace(xmin, xmax, 100)\n",
    "xi = numpy.linspace(xmin, xmax, N)\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "axes = fig.add_subplot(1, 2, 1)\n",
    "axes.plot(xa, func(xa), 'b', label=\"$f(x)$\")\n",
    "axes.plot(xa, func_prime(xa), 'k--', label=\"$f'(x)$\")\n",
    "axes.plot(xi, func(xi), 'ro')\n",
    "axes.plot(xi, D1f, 'ko',label='$D^+_1(f)$')\n",
    "axes.legend(loc='best')\n",
    "axes.set_title(\"$f'(x)$\")\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"$f'(x)$ and $\\hat{f}'(x)$\")\n",
    "axes.grid()\n",
    "\n",
    "axes = fig.add_subplot(1, 2, 2)\n",
    "axes.plot(xa, func(xa), 'b', label=\"$f(x)$\")\n",
    "axes.plot(xa, func_prime(xa), 'k--', label=\"$f'(x)$\")\n",
    "axes.plot(xi, func(xi), 'ro')\n",
    "axes.plot(xi, D2f, 'go',label='$D_1^2(f)$')\n",
    "axes.legend(loc='best')\n",
    "axes.set_title(\"$f'(x)$\")\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"$f'(x)$ and $\\hat{f}'(x)$\")\n",
    "axes.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N = 21\n",
    "xmin = -1\n",
    "xmax = 1.\n",
    "func = lambda x:  1./(1 + 25.*x**2)\n",
    "func_prime = lambda x: -50. * x / (1. + 25.*x**2)**2\n",
    "D1f = D1_p(func, xmin, xmax, N)\n",
    "D2f = D1_2(func, xmin, xmax, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "xa = numpy.linspace(xmin, xmax, 100)\n",
    "xi = numpy.linspace(xmin, xmax, N)\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "axes = fig.add_subplot(1, 2, 1)\n",
    "axes.plot(xa, func(xa), 'b', label=\"$f(x)$\")\n",
    "axes.plot(xa, func_prime(xa), 'k--', label=\"$f'(x)$\")\n",
    "axes.plot(xi, func(xi), 'ro')\n",
    "axes.plot(xi, D1f, 'ko',label='$D^+_1(f)$')\n",
    "axes.legend(loc='best')\n",
    "axes.set_title(\"$f'(x)$\")\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"$f'(x)$ and $\\hat{f}'(x)$\")\n",
    "axes.grid()\n",
    "\n",
    "axes = fig.add_subplot(1, 2, 2)\n",
    "axes.plot(xa, func(xa), 'b', label=\"$f(x)$\")\n",
    "axes.plot(xa, func_prime(xa), 'k--', label=\"$f'(x)$\")\n",
    "axes.plot(xi, func(xi), 'ro')\n",
    "axes.plot(xi, D2f, 'go',label='$D_1^2(f)$')\n",
    "axes.legend(loc='best')\n",
    "axes.set_title(\"$f'(x)$\")\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"$f'(x)$ and $\\hat{f}'(x)$\")\n",
    "axes.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Computing Order of Convergence\n",
    "\n",
    "Say we had the error $E(\\Delta x)$ and we wanted to make a statement about the rate of convergence (note we can replace $E$ here with the $R$ from above).  Then we can do the following:\n",
    "$$\\begin{aligned}\n",
    "    E(\\Delta x) &= C \\Delta x^n \\\\\n",
    "    \\log E(\\Delta x) &= \\log C + n \\log \\Delta x\n",
    "\\end{aligned}$$\n",
    "\n",
    "The slope of the line is $n$ when modeling the error like this!  We can also match the first point by solving for $C$:\n",
    "\n",
    "$$\n",
    "    C = e^{\\log E(\\Delta x) - n \\log \\Delta x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the error as a function of delta_x\n",
    "N_range = numpy.logspace(1, 4, 10, dtype=int)\n",
    "delta_x = numpy.empty(N_range.shape)\n",
    "error = numpy.empty((N_range.shape[0], 4))\n",
    "for (i, N) in enumerate(N_range):\n",
    "    x_hat = numpy.linspace(xmin, xmax, N)\n",
    "    delta_x[i] = x_hat[1] - x_hat[0]\n",
    "\n",
    "    # Compute forward difference\n",
    "    D1f = D1_p(func, xmin, xmax, N)\n",
    "    \n",
    "    # Compute 2nd order difference\n",
    "    D2f = D1_2(func, xmin, xmax, N)\n",
    "\n",
    "    \n",
    "    # Calculate the infinity norm or maximum error\n",
    "    error[i, 0] = numpy.linalg.norm(numpy.abs(func_prime(x_hat) - D1f), ord=numpy.inf)\n",
    "    error[i, 1] = numpy.linalg.norm(numpy.abs(func_prime(x_hat) - D2f), ord=numpy.inf)\n",
    "    \n",
    "error = numpy.array(error)\n",
    "delta_x = numpy.array(delta_x)\n",
    "  \n",
    "order_C = lambda delta_x, error, order: numpy.exp(numpy.log(error) - order * numpy.log(delta_x))\n",
    "    \n",
    "fig = plt.figure(figsize=(8,6))\n",
    "axes = fig.add_subplot(1,1,1)\n",
    "axes.loglog(delta_x, error[:,0], 'ro', label='$D_1^+$')\n",
    "axes.loglog(delta_x, error[:,1], 'bo', label='$D_1^2$')\n",
    "axes.loglog(delta_x, order_C(delta_x[0], error[0, 0], 1.0) * delta_x**1.0, 'r--', label=\"1st Order\")\n",
    "axes.loglog(delta_x, order_C(delta_x[0], error[0, 1], 2.0) * delta_x**2.0, 'b--', label=\"2nd Order\")\n",
    "axes.legend(loc=4)\n",
    "axes.set_title(\"Convergence of Finite Differences\", fontsize=18)\n",
    "axes.set_xlabel(\"$\\Delta x$\", fontsize=16)\n",
    "axes.set_ylabel(\"$|f'(x) - \\hat{f}'(x)|$\", fontsize=16)\n",
    "axes.legend(loc='best', fontsize=14)\n",
    "axes.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Computing Order of Convergence\n",
    "\n",
    "Alternatively,  we can fit our data to the best fit line in log-log space to estimate the rate of convergence $n$. However, as these estimates are always asymptotic in the limit of small $\\Delta x$, it is often most instructive to just fit the small $\\Delta x$ tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the largest value of $\\Delta_x$ for fitting\n",
    "N_max = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the error as a function of delta_x\n",
    "N_range = numpy.logspace(1, 4, 10, dtype=int)\n",
    "delta_x = numpy.empty(N_range.shape)\n",
    "error = numpy.empty((N_range.shape[0], 4))\n",
    "for (i, N) in enumerate(N_range):\n",
    "    x_hat = numpy.linspace(xmin, xmax, N)\n",
    "    delta_x[i] = x_hat[1] - x_hat[0]\n",
    "\n",
    "    # Compute forward difference\n",
    "    D1f = D1_p(func, xmin, xmax, N)\n",
    "    \n",
    "    # Compute 2nd order difference\n",
    "    D2f = D1_2(func, xmin, xmax, N)\n",
    "\n",
    "    \n",
    "    # Calculate the infinity norm or maximum error\n",
    "    error[i, 0] = numpy.linalg.norm(numpy.abs(func_prime(x_hat) - D1f), ord=numpy.inf)\n",
    "    error[i, 1] = numpy.linalg.norm(numpy.abs(func_prime(x_hat) - D2f), ord=numpy.inf)\n",
    "    \n",
    "error = numpy.array(error)\n",
    "delta_x = numpy.array(delta_x)\n",
    "  \n",
    "# calculate linear fits to log(err) v. log(\\Delta x)\n",
    "p1 = numpy.polyfit(numpy.log(delta_x[N_max:]),numpy.log(error[N_max:,0]),1)\n",
    "p2 = numpy.polyfit(numpy.log(delta_x[N_max:]),numpy.log(error[N_max:,1]),1)\n",
    "#print(p1,p2)\n",
    "\n",
    "fit = lambda delta_x,  p: numpy.exp(p[1])*delta_x**p[0] \n",
    "\n",
    "\n",
    "    \n",
    "fig = plt.figure(figsize=(8,6))\n",
    "axes = fig.add_subplot(1,1,1)\n",
    "axes.loglog(delta_x, error[:,0], 'ro', label='$D_1^+$')\n",
    "axes.loglog(delta_x, error[:,1], 'bo', label='$D_1^2$')\n",
    "axes.loglog(delta_x, fit(delta_x, p1), 'r--', label=\"n={:.2f}\".format(p1[0]))\n",
    "axes.loglog(delta_x, fit(delta_x, p2), 'b--', label=\"n={:.2f}\".format(p2[0]))\n",
    "axes.loglog(delta_x[N_max],error[N_max,0],'rs',markersize=15,fillstyle='none')\n",
    "axes.loglog(delta_x[N_max],error[N_max,1],'bs',markersize=15,fillstyle='none')\n",
    "\n",
    "axes.legend(loc=4)\n",
    "axes.set_title(\"Convergence of Finite Differences\", fontsize=18)\n",
    "axes.set_xlabel(\"$\\Delta x$\", fontsize=16)\n",
    "axes.set_ylabel(\"$|f'(x) - \\hat{f}'(x)|$\", fontsize=16)\n",
    "axes.legend(loc='best', fontsize=14)\n",
    "axes.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Another approach:  The method of undetermined Coefficients\n",
    "\n",
    "An alternative method for finding finite-difference formulas is by using Taylor series expansions about the point we want to approximate.  The Taylor series about $x_n$ is\n",
    "\n",
    "$$f(x) = f(x_n) + (x - x_n) f'(x_n) + \\frac{(x - x_n)^2}{2!} f''(x_n) + \\frac{(x - x_n)^3}{3!} f'''(x_n) + \\mathcal{O}((x - x_n)^4)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Say we want to derive the second order accurate, first derivative approximation that we just did, this requires the values $(x_{n+1}, f(x_{n+1})$ and $(x_{n-1}, f(x_{n-1})$.  We can express these values via our Taylor series approximation above as\n",
    "\n",
    "\\begin{aligned}\n",
    "    f(x_{n+1}) &= f(x_n) + (x_{n+1} - x_n) f'(x_n) + \\frac{(x_{n+1} - x_n)^2}{2!} f''(x_n) + \\frac{(x_{n+1} - x_n)^3}{3!} f'''(x_n) + \\mathcal{O}((x_{n+1} - x_n)^4) \\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "or\n",
    "\\begin{aligned}\n",
    "&= f(x_n) + \\Delta x f'(x_n) + \\frac{\\Delta x^2}{2!} f''(x_n) + \\frac{\\Delta x^3}{3!} f'''(x_n) + \\mathcal{O}(\\Delta x^4)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "and\n",
    "\n",
    "\\begin{align}\n",
    "f(x_{n-1}) &= f(x_n) + (x_{n-1} - x_n) f'(x_n) + \\frac{(x_{n-1} - x_n)^2}{2!} f''(x_n) + \\frac{(x_{n-1} - x_n)^3}{3!} f'''(x_n) + \\mathcal{O}((x_{n-1} - x_n)^4) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{align} \n",
    "&= f(x_n) - \\Delta x f'(x_n) + \\frac{\\Delta x^2}{2!} f''(x_n) - \\frac{\\Delta x^3}{3!} f'''(x_n) + \\mathcal{O}(\\Delta x^4)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or all together (for regularly spaced points),\n",
    "\\begin{align} \n",
    "f(x_{n+1}) &= f(x_n) + \\Delta x f'(x_n) + \\frac{\\Delta x^2}{2!} f''(x_n) + \\frac{\\Delta x^3}{3!} f'''(x_n) + \\mathcal{O}(\\Delta x^4)\\\\\n",
    "f(x_n) &= f(x_n) \\\\\n",
    "f(x_{n-1})&= f(x_n) - \\Delta x f'(x_n) + \\frac{\\Delta x^2}{2!} f''(x_n) - \\frac{\\Delta x^3}{3!} f'''(x_n) + \\mathcal{O}(\\Delta x^4)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now to find out how to combine these into an expression for the derivative we assume our approximation looks like\n",
    "\n",
    "$$\n",
    "    f'(x_n) + R(x_n) = A f(x_{n+1}) + B f(x_n) + C f(x_{n-1})\n",
    "$$\n",
    "\n",
    "where $R(x_n)$ is our error, and $A,B,C$ are our ``undetermined coefficients''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Plugging in the Taylor series approximations we find\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    f'(x_n) + R(x_n) &= A \\left ( f(x_n) + \\Delta x f'(x_n) + \\frac{\\Delta x^2}{2!} f''(x_n) + \\frac{\\Delta x^3}{3!} f'''(x_n) + \\mathcal{O}(\\Delta x^4)\\right ) \\\\\n",
    "    & + B  ~~~~f(x_n)  \\\\ \n",
    "    & + C \\left ( f(x_n) - \\Delta x f'(x_n) + \\frac{\\Delta x^2}{2!} f''(x_n) - \\frac{\\Delta x^3}{3!} f'''(x_n) + \\mathcal{O}(\\Delta x^4) \\right )\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or\n",
    "$$\n",
    "f'(x_n) + R(x_n)= (A + B + C) f(x_n) + (A\\Delta x +0B - C\\Delta x)f'(x_n) + (A\\frac{\\Delta x^2}{2!} + C\\frac{\\Delta x^2}{2!})f''(x_n) + O(\\Delta x^3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since we want $R(x_n) = \\mathcal{O}(\\Delta x^2)$ we want all terms lower than this to cancel except for those multiplying $f'(x_n)$ as those should sum to 1 to give us our approximation.  Collecting the terms with common evaluations of the derivatives on $f(x_n)$ we get a series of expressions for the coefficients $A$, $B$, and $C$ based on the fact we want an approximation to $f'(x_n)$.  The $n=0$ terms collected are $A + B + C$ and are set to 0 as we want the $f(x_n)$ term to also cancel.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    f(x_n):&  &A + B + C &= 0 \\\\\n",
    "    f'(x_n): & &A \\Delta x - C \\Delta x &= 1 \\\\\n",
    "    f''(x_n): & &A \\frac{\\Delta x^2}{2} + C \\frac{\\Delta x^2}{2} &= 0\n",
    "\\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or as a linear algebra problem\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "\\Delta x & 0 &-\\Delta x \\\\\n",
    "\\frac{\\Delta x^2}{2} & 0 & \\frac{\\Delta x^2}{2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} A \\\\ B\\\\ C\\\\\\end{bmatrix} =\n",
    "\\begin{bmatrix} 0 \\\\ 1\\\\ 0\\\\\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "This last equation $\\Rightarrow A = -C$, using this in the second equation gives $A = \\frac{1}{2 \\Delta x}$ and $C = -\\frac{1}{2 \\Delta x}$.  The first equation then leads to $B = 0$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Putting this altogether then gives us our previous expression including an estimate for the error:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    f'(x_n) + R(x_n) &= \\quad \\frac{1}{2 \\Delta x} \\left ( f(x_n) + \\Delta x f'(x_n) + \\frac{\\Delta x^2}{2!} f''(x_n) + \\frac{\\Delta x^3}{3!} f'''(x_n) + \\mathcal{O}(\\Delta x^4)\\right ) \\\\\n",
    "    & \\quad + 0 \\cdot f(x_n) \\\\ \n",
    "    & \\quad - \\frac{1}{2 \\Delta x} \\left ( f(x_n) - \\Delta x f'(x_n) + \\frac{\\Delta x^2}{2!} f''(x_n) - \\frac{\\Delta x^3}{3!} f'''(x_n) + \\mathcal{O}(\\Delta x^4) \\right ) \\\\\n",
    "    &=  f'(x_n) + \\frac{1}{2 \\Delta x} \\left ( \\frac{2 \\Delta x^3}{3!} f'''(x_n) + \\mathcal{O}(\\Delta x^4)\\right )\n",
    "\\end{aligned}$$\n",
    "so that we find\n",
    "$$\n",
    "    R(x_n) = \\frac{\\Delta x^2}{3!} f'''(x_n) + \\mathcal{O}(\\Delta x^3) = \\mathcal{O}(\\Delta x^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Another way...\n",
    "\n",
    "There is one more way to derive the second order accurate, first order finite-difference formula.  Consider the two first order forward and backward finite-differences averaged together:\n",
    "\n",
    "$$\\frac{D_1^+(f(x_n)) + D_1^-(f(x_n))}{2} = \\frac{f(x_{n+1}) - f(x_n) + f(x_n) - f(x_{n-1})}{2 \\Delta x} = \\frac{f(x_{n+1}) - f(x_{n-1})}{2 \\Delta x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example 4: Higher Order Derivatives\n",
    "\n",
    "Using our Taylor series approach lets derive the second order accurate second derivative formula.  Again we will use the same points and the Taylor series centered at $x = x_n$ so we end up with the same expression as before:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    f''(x_n) + R(x_n) &= \\quad A \\left ( f(x_n) + \\Delta x f'(x_n) + \\frac{\\Delta x^2}{2!} f''(x_n) + \\frac{\\Delta x^3}{3!} f'''(x_n) + \\frac{\\Delta x^4}{4!} f^{(4)}(x_n) + \\mathcal{O}(\\Delta x^5)\\right ) \\\\\n",
    "    &+ \\quad B \\cdot f(x_n) \\\\\n",
    "    &+ \\quad C \\left ( f(x_n) - \\Delta x f'(x_n) + \\frac{\\Delta x^2}{2!} f''(x_n) - \\frac{\\Delta x^3}{3!} f'''(x_n) + \\frac{\\Delta x^4}{4!} f^{(4)}(x_n) + \\mathcal{O}(\\Delta x^5) \\right )\n",
    "\\end{aligned}$$\n",
    "\n",
    "except this time we want to leave $f''(x_n)$ on the right hand side.  \n",
    "\n",
    "Try out the same trick as before and see if you can setup the equations that need to be solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Doing the same trick as before we have the following expressions:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    f(x_n): & & A + B + C &= 0\\\\\n",
    "    f'(x_n): & & A \\Delta x - C \\Delta x &= 0\\\\\n",
    "    f''(x_n): & & A \\frac{\\Delta x^2}{2} + C \\frac{\\Delta x^2}{2} &= 1\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or again\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "\\Delta x & 0 &-\\Delta x \\\\\n",
    "\\frac{\\Delta x^2}{2} & 0 & \\frac{\\Delta x^2}{2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} A \\\\ B\\\\ C\\\\\\end{bmatrix} =\n",
    "\\begin{bmatrix} 0 \\\\ 0\\\\ 1\\\\\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Note,  the Matrix remains, the same, only the right hand side has changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The second equation implies $A = C$ which combined with the third implies\n",
    "\n",
    "$$A = C = \\frac{1}{\\Delta x^2}$$\n",
    "\n",
    "Finally the first equation gives\n",
    "\n",
    "$$B = -\\frac{2}{\\Delta x^2}$$\n",
    "\n",
    "leading to the final expression\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    f''(x_n) + R(x_n) &= \\quad \\frac{1}{\\Delta x^2} \\left ( f(x_n) + \\Delta x f'(x_n) + \\frac{\\Delta x^2}{2!} f''(x_n) + \\frac{\\Delta x^3}{3!} f'''(x_n) + \\frac{\\Delta x^4}{4!} f^{(4)}(x_n) + \\mathcal{O}(\\Delta x^5)\\right ) \\\\\n",
    "    &+ \\quad -\\frac{2}{\\Delta x^2} \\cdot f(x_n) \\\\\n",
    "    &+ \\quad \\frac{1}{\\Delta x^2} \\left ( f(x_n) - \\Delta x f'(x_n) + \\frac{\\Delta x^2}{2!} f''(x_n) - \\frac{\\Delta x^3}{3!} f'''(x_n) + \\frac{\\Delta x^4}{4!} f^{(4)}(x_n) + \\mathcal{O}(\\Delta x^5) \\right ) \\\\\n",
    "    &= f''(x_n) + \\frac{1}{\\Delta x^2} \\left(\\frac{2 \\Delta x^4}{4!} f^{(4)}(x_n) + \\mathcal{O}(\\Delta x^5) \\right )\n",
    "\\end{aligned}\n",
    "$$\n",
    "so that\n",
    "\n",
    "$$\n",
    "    R(x_n) = \\frac{\\Delta x^2}{12} f^{(4)}(x_n) + \\mathcal{O}(\\Delta x^3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def D2(func, x_min, x_max, N):\n",
    "    \"\"\" calculate consistent 2nd order second derivative of a function func(x) defined on the interval [x_min,xmax]\n",
    "    and sampled at N evenly spaced points\"\"\"\n",
    "\n",
    "    x = numpy.linspace(x_min, x_max, N)\n",
    "    f = func(x)\n",
    "    dx = x[1] - x[0]\n",
    "    D2f = numpy.zeros(x.shape) \n",
    "    D2f[1:-1] = (f[:-2] - 2*f[1:-1] + f[2:])/(dx**2)\n",
    "    # patch up end points to be 1 sided 2nd derivatives\n",
    "    D2f[0] = D2f[1]\n",
    "    D2f[-1] = D2f[-2]\n",
    "\n",
    "    \n",
    "    return D2f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f = lambda x: numpy.sin(x)\n",
    "f_dubl_prime = lambda x: -numpy.sin(x)\n",
    "\n",
    "# Use uniform discretization\n",
    "x = numpy.linspace(-2 * numpy.pi, 2 * numpy.pi, 1000)\n",
    "N = 80\n",
    "x_hat = numpy.linspace(-2 * numpy.pi, 2 * numpy.pi, N)\n",
    "delta_x = x_hat[1] -  x_hat[0]\n",
    "\n",
    "# Compute derivative\n",
    "D2f  = D2(f, x_hat[0], x_hat[-1], N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.plot(x,f(x),'b',label='$f(x)$')\n",
    "axes.plot(x, f_dubl_prime(x), 'k--', label=\"$f'(x)$\")\n",
    "axes.plot(x_hat, D2f, 'ro', label='$D_2(f)$')\n",
    "axes.set_xlim((x[0], x[-1]))\n",
    "axes.set_ylim((-1.1, 1.1))\n",
    "axes.legend(loc='best',fontsize=14)\n",
    "axes.grid()\n",
    "axes.set_title('Discrete Second derivative',fontsize=18)\n",
    "axes.set_xlabel('$x$', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The general case\n",
    "\n",
    "In the general case we can use any $N+1$ points to calculate consistent finite difference coefficients for approximating any derivative of order $k \\leq N$.  Relaxing the requirement of equal grid spacing (or the expectation that the location where the derivative is evaluated  $\\bar{x}$, is one of the grid points) the Taylor series expansions become\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    f^{(k)}(\\bar{x}) + R(\\bar{x}) &= \\quad c_0 \\left ( f(\\bar{x}) + \\Delta x_0 f'(\\bar{x}) + \\frac{\\Delta x_0^2}{2!} f''(\\bar{x}) + \\frac{\\Delta x_0^3}{3!} f'''(\\bar{x}) + \\frac{\\Delta x_0^4}{4!} f^{(4)}(\\bar{x}) + \\mathcal{O}(\\Delta x_0^5)\\right ) \\\\\n",
    "    &+ \\quad c_1 \\left ( f(\\bar{x}) + \\Delta x_1 f'(\\bar{x}) + \\frac{\\Delta x_1^2}{2!} f''(\\bar{x}) + \\frac{\\Delta x_1^3}{3!} f'''(\\bar{x}) + \\frac{\\Delta x_1^4}{4!} f^{(4)}(\\bar{x}) + \\mathcal{O}(\\Delta x_1^5)\\right )\\\\\n",
    "    &+ \\quad c_2 \\left ( f(\\bar{x}) + \\Delta x_2 f'(\\bar{x}) + \\frac{\\Delta x_2^2}{2!} f''(\\bar{x}) + \\frac{\\Delta x_2^3}{3!} f'''(\\bar{x}) + \\frac{\\Delta x_2^4}{4!} f^{(4)}(\\bar{x}) + \\mathcal{O}(\\Delta x_2^5)\\right ) \\\\\n",
    "    &+ \\quad \\vdots\\\\\n",
    "    &+ \\quad c_N \\left ( f(\\bar{x}) + \\Delta x_N f'(\\bar{x}) + \\frac{\\Delta x_N^2}{2!} f''(\\bar{x}) + \\frac{\\Delta x_N^3}{3!} f'''(\\bar{x}) + \\frac{\\Delta x_N^4}{4!} f^{(4)}(\\bar{x}) + \\mathcal{O}(\\Delta x_N^5)\\right ) \\\\\n",
    "\\end{aligned}$$\n",
    "where $\\Delta\\mathbf{x} = \\bar{x} - \\mathbf{x}$ is the distance between the point $\\bar{x}$ and each grid point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Equating terms of equal order reduces the problem to another Vandermonde matrix problem\n",
    "$$\\begin{bmatrix}\n",
    "1 & 1 & 1 & \\cdots & 1 \\\\\n",
    "\\Delta x_0 & \\Delta x_1 & \\Delta x_2 & \\cdots & \\Delta x_N\\\\\n",
    "\\frac{\\Delta x_0^2}{2!} & \\frac{\\Delta x_1^2}{2!} & \\frac{\\Delta x_2^2}{2!} &\\cdots &  \\frac{\\Delta x_N^2}{2!}\\\\\n",
    " &  & \\vdots & \\cdots &  \\\\\n",
    "\\frac{\\Delta x_0^N}{N!} & \\frac{\\Delta x_1^N}{N!} & \\frac{\\Delta x_2^N}{N!} & \\cdots &  \\frac{\\Delta x_N^N}{N!}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} c_0 \\\\ c_1\\\\ c_2 \\\\ \\vdots \\\\ c_N\\\\\\end{bmatrix} =\n",
    "\\mathbf{b}_k \n",
    "$$\n",
    "\n",
    "where $\\mathbf{b}_k$ is a vector of zeros with just a one in the $k$th position for the $k$th derivative.\n",
    "\n",
    "By exactly accounting for the first $N+1$ terms of the Taylor series (with $N+1$ equations),  we can get any order derivative $0<k<N$ as well as an Error estimate for \n",
    "\n",
    "$$R(\\bar{x}) = O\\left(\\frac{(\\mathbf{c}^T\\mathbf{1})\\Delta{x}^{N+1}}{(N+1)!}f^{(N+1)}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This approach of \"undetermined coefficients\" can be efficiently coded up as a routine to provide consistent $Nth$ order finite difference coefficients for an arbitrarily spaced grid $\\mathbf{x}$.  \n",
    "\n",
    "Here we present a python version of a matlab routine `fdcoeffV.m` from Randy Leveque's excellent book [Finite Difference Methods for ordinary and partial differential equations](https://faculty.washington.edu/rjl/fdmbook/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def fdcoeffV(k,xbar,x):\n",
    "    \"\"\"\n",
    "    fdcoeffV routine modified from Leveque (2007) matlab function\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    \n",
    "    k: int\n",
    "        order of derivative\n",
    "    xbar: float\n",
    "        point at which derivative is to be evaluated\n",
    "    x: ndarray\n",
    "        numpy array of coordinates to use in calculating the weights\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    c: ndarray\n",
    "        array of floats of coefficients.  \n",
    "\n",
    "    Compute coefficients for finite difference approximation for the\n",
    "    derivative of order k at xbar based on grid values at points in x.\n",
    "\n",
    "    WARNING: This approach is numerically unstable for large values of n since\n",
    "    the Vandermonde matrix is poorly conditioned.  Use fdcoeffF.m instead,\n",
    "    which is based on Fornberg's method.\n",
    "\n",
    "     This function returns a row vector c of dimension 1 by n, where n=length(x),\n",
    "     containing coefficients to approximate u^{(k)}(xbar), \n",
    "     the k'th derivative of u evaluated at xbar,  based on n values\n",
    "     of u at x(1), x(2), ... x(n).  \n",
    "\n",
    "     If U is an array containing u(x) at these n points, then \n",
    "     c.dot(U) will give the approximation to u^{(k)}(xbar).\n",
    "\n",
    "     Note for k=0 this can be used to evaluate the interpolating polynomial \n",
    "     itself.\n",
    "\n",
    "    Requires len(x) > k.  \n",
    "    Usually the elements x(i) are monotonically increasing\n",
    "    and x(1) <= xbar <= x(n), but neither condition is required.\n",
    "    The x values need not be equally spaced but must be distinct.  \n",
    "    \n",
    "    Modified rom  http://www.amath.washington.edu/~rjl/fdmbook/  (2007)\n",
    "    \"\"\"\n",
    "    \n",
    "    from  scipy.special import factorial\n",
    "\n",
    "    n = x.shape[0]\n",
    "    assert  k < n, \" The order of the derivative must be less than the stencil width\"\n",
    "\n",
    "    # Generate the Vandermonde matrix from the Taylor series\n",
    "    A = numpy.ones((n,n))\n",
    "    xrow = (x - xbar)  # displacements x-xbar \n",
    "    for i in range(1,n):\n",
    "        A[i,:] = (xrow**(i))/factorial(i);\n",
    "        \n",
    "    b = numpy.zeros(n)    # b is right hand side,\n",
    "    b[k] = 1              # so k'th derivative term remains\n",
    "\n",
    "    c = numpy.linalg.solve(A,b)          # solve n by n system for coefficients\n",
    "    \n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N = 11\n",
    "x = numpy.linspace(-2*numpy.pi, 2.*numpy.pi, N )\n",
    "k = 1\n",
    "scale = (x[1]-x[0])**k\n",
    "\n",
    "print(fdcoeffV(k,x[0],x[:3])*scale)\n",
    "for j in range(k,N-1):\n",
    "    print(fdcoeffV(k,  x[j], x[j-1:j+2])*scale)\n",
    "\n",
    "print(fdcoeffV(k,x[-1],x[-3:])*scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: A variably spaced mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "N = 21\n",
    "y = numpy.linspace(-.95, .95,N)\n",
    "x = numpy.arctanh(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(x,numpy.zeros(x.shape),'bo-')\n",
    "axes.plot(x,y,'ro-')\n",
    "axes.grid()\n",
    "axes.set_xlabel('$x$')\n",
    "axes.set_ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1  \n",
    "fd = fdcoeffV(k,x[0],x[:3])\n",
    "print('{}, sum={}'.format(fd,fd.sum()))\n",
    "for j in range(1,N-1):\n",
    "    fd = fdcoeffV(k,  x[j], x[j-1:j+2])\n",
    "    print('{}, sum={}'.format(fd,fd.sum()))\n",
    "fd = fdcoeffV(k,x[-1],x[-3:])\n",
    "print('{}, sum={}'.format(fd,fd.sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Application to Numerical PDE's\n",
    "\n",
    "Given an efficent way to generate Finite Difference Coefficients these coefficients can be stored in a (usually sparse) matrix $D_k$ such that  given any discrete vector $\\mathbf{f} = f(\\mathbf{x})$,  We can calculate the approximate $k$th derivative as simply the matrix vector product\n",
    "\n",
    "$$\n",
    "    \\mathbf{f}' = D_k\\mathbf{f}\n",
    "$$\n",
    "\n",
    "This technique will become extremely useful when solving basic finite difference approximations to differential equations (as we will explore in future lectures and homeworks).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Bigger idea\n",
    "\n",
    "More generally, using finite differences we can transform a continuous differential operator on a function space\n",
    "\n",
    "$$\n",
    "    v = \\frac{d}{dx} u(x)\n",
    "$$\n",
    "which maps a function to a function,  to a discrete linear algebraic problem \n",
    "\n",
    "$$\n",
    "    \\mathbf{v} = D\\mathbf{u}\n",
    "$$\n",
    "where $\\mathbf{v}, \\mathbf{u}$ are discrete approximations to the continous functions $v,u$ and $D$ is a discrete differential operator (Matrix) which maps a vector to a vector."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
